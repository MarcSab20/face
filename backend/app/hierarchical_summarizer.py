"""
Service de R√©sum√© Hi√©rarchique Intelligent
R√©sout le probl√®me de contexte limit√© en r√©sumant par lots puis en agr√©geant
"""

import logging
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import asyncio
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class ContentBatch:
    """Un lot de contenus √† r√©sumer"""
    batch_id: int
    contents: List[Dict]
    summary: Optional[str] = None
    key_points: Optional[List[str]] = None
    sentiment_aggregate: Optional[Dict] = None


@dataclass
class HierarchicalSummary:
    """R√©sum√© hi√©rarchique complet"""
    final_summary: str
    key_insights: List[str]
    sentiment_analysis: Dict
    themes: List[str]
    batch_summaries: List[str]
    total_contents_analyzed: int
    processing_time: float


class HierarchicalSummarizer:
    """
    R√©sumeur hi√©rarchique intelligent
    
    Principe:
    1. Diviser le contenu en lots g√©rables (batch)
    2. R√©sumer chaque lot individuellement
    3. Agr√©ger les r√©sum√©s de lots
    4. G√©n√©rer la synth√®se finale
    
    Permet de traiter des milliers de documents sans d√©passer les limites de contexte
    """
    
    def __init__(
        self,
        llm_service,  # Service LLM (Ollama, Gemini, Groq, etc.)
        batch_size: int = 20,  # Nombre de contenus par lot
        max_content_length: int = 500  # Taille max par contenu
    ):
        self.llm_service = llm_service
        self.batch_size = batch_size
        self.max_content_length = max_content_length
        logger.info(f"HierarchicalSummarizer initialis√© (batch_size={batch_size})")
    
    async def summarize_large_dataset(
        self,
        contents: List[Dict],
        context: str = "analyse g√©n√©rale"
    ) -> HierarchicalSummary:
        """
        R√©sumer un grand ensemble de donn√©es de mani√®re hi√©rarchique
        
        Args:
            contents: Liste de contenus (posts, commentaires, etc.)
            context: Contexte de l'analyse
            
        Returns:
            R√©sum√© hi√©rarchique complet
        """
        start_time = datetime.utcnow()
        logger.info(f"üìä D√©marrage r√©sum√© hi√©rarchique: {len(contents)} contenus")
        
        if not contents:
            return self._empty_summary()
        
        # √âTAPE 1: Diviser en lots
        batches = self._create_batches(contents)
        logger.info(f"   ‚úì Divis√© en {len(batches)} lots de ~{self.batch_size} contenus")
        
        # √âTAPE 2: R√©sumer chaque lot en parall√®le
        logger.info(f"   üîÑ R√©sum√© des lots en cours...")
        batch_summaries = await self._summarize_batches(batches, context)
        logger.info(f"   ‚úì {len(batch_summaries)} lots r√©sum√©s")
        
        # √âTAPE 3: Agr√©ger les sentiments
        sentiment_aggregate = self._aggregate_sentiments(contents)
        
        # √âTAPE 4: Extraire les th√®mes dominants
        themes = self._extract_themes(batch_summaries)
        
        # √âTAPE 5: Synth√®se finale des r√©sum√©s de lots
        logger.info(f"   üéØ G√©n√©ration synth√®se finale...")
        final_summary, key_insights = await self._generate_final_synthesis(
            batch_summaries,
            sentiment_aggregate,
            themes,
            context
        )
        
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        logger.info(f"‚úÖ R√©sum√© hi√©rarchique termin√© en {processing_time:.1f}s")
        
        return HierarchicalSummary(
            final_summary=final_summary,
            key_insights=key_insights,
            sentiment_analysis=sentiment_aggregate,
            themes=themes,
            batch_summaries=[b.summary for b in batches if b.summary],
            total_contents_analyzed=len(contents),
            processing_time=processing_time
        )
    
    def _create_batches(self, contents: List[Dict]) -> List[ContentBatch]:
        """Diviser les contenus en lots g√©rables"""
        batches = []
        
        for i in range(0, len(contents), self.batch_size):
            batch_contents = contents[i:i + self.batch_size]
            
            batches.append(ContentBatch(
                batch_id=len(batches) + 1,
                contents=batch_contents
            ))
        
        return batches
    
    async def _summarize_batches(
        self,
        batches: List[ContentBatch],
        context: str
    ) -> List[ContentBatch]:
        """R√©sumer tous les lots en parall√®le"""
        
        tasks = [
            self._summarize_single_batch(batch, context)
            for batch in batches
        ]
        
        # Ex√©cuter en parall√®le avec limite de concurrence
        semaphore = asyncio.Semaphore(3)  # Max 3 r√©sum√©s simultan√©s
        
        async def bounded_task(task):
            async with semaphore:
                return await task
        
        summarized_batches = await asyncio.gather(
            *[bounded_task(task) for task in tasks],
            return_exceptions=True
        )
        
        # Filtrer les erreurs
        valid_batches = [
            batch for batch in summarized_batches
            if not isinstance(batch, Exception) and batch.summary
        ]
        
        return valid_batches
    
    async def _summarize_single_batch(
        self,
        batch: ContentBatch,
        context: str
    ) -> ContentBatch:
        """
        R√©sumer un seul lot de contenus
        
        Le r√©sum√© doit capturer l'essentiel en quelques phrases
        """
        
        # Pr√©parer le texte du lot
        batch_text = self._format_batch_for_summarization(batch.contents)
        
        prompt = f"""R√©sume ce lot de {len(batch.contents)} contenus sur '{context}'.

CONTENUS:
{batch_text}

Fournis un r√©sum√© en 3-5 phrases capturant:
1. Les id√©es principales exprim√©es
2. Le ton g√©n√©ral (positif/n√©gatif/neutre)
3. Les points de discussion r√©currents

Sois factuel et concis. NE PAS mentionner "lot" ou "batch".

R√âSUM√â:"""
        
        try:
            # Utiliser le service LLM
            response = await self.llm_service.analyze_with_local_llm(
                prompt,
                {'batch_size': len(batch.contents)}
            )
            
            # Nettoyer la r√©ponse
            summary = self._clean_summary(response)
            
            # Extraire les points cl√©s
            key_points = self._extract_key_points(batch.contents, summary)
            
            batch.summary = summary
            batch.key_points = key_points
            
            return batch
            
        except Exception as e:
            logger.error(f"Erreur r√©sum√© lot {batch.batch_id}: {e}")
            # Fallback: r√©sum√© basique par r√®gles
            batch.summary = self._fallback_batch_summary(batch.contents)
            return batch
    
    def _format_batch_for_summarization(self, contents: List[Dict]) -> str:
        """Formater un lot de contenus pour le r√©sum√©"""
        
        formatted_lines = []
        
        for i, content in enumerate(contents[:self.batch_size], 1):
            # Extraire le texte principal
            title = content.get('title', '')
            text = content.get('content', '') or content.get('text', '')
            author = content.get('author', 'Anonyme')
            
            # Limiter la taille
            combined_text = f"{title} {text}"[:self.max_content_length]
            
            formatted_lines.append(f"{i}. [{author}] {combined_text}")
        
        return '\n'.join(formatted_lines)
    
    def _clean_summary(self, raw_summary: str) -> str:
        """Nettoyer le r√©sum√© g√©n√©r√© par le LLM"""
        import re
        
        # Enlever les pr√©fixes communs
        summary = re.sub(r'^(R√©sum√©|Summary|R√âSUM√â):\s*', '', raw_summary, flags=re.IGNORECASE)
        
        # Enlever les balises markdown restantes
        summary = re.sub(r'\*\*([^*]+)\*\*', r'\1', summary)
        summary = re.sub(r'\*([^*]+)\*', r'\1', summary)
        
        # Nettoyer les espaces multiples
        summary = re.sub(r'\s+', ' ', summary)
        
        return summary.strip()
    
    def _extract_key_points(self, contents: List[Dict], summary: str) -> List[str]:
        """Extraire les points cl√©s d'un lot"""
        
        key_points = []
        
        # Points bas√©s sur l'engagement
        high_engagement = sorted(
            contents,
            key=lambda x: x.get('engagement_score', 0),
            reverse=True
        )[:3]
        
        for content in high_engagement:
            title = content.get('title', '')
            if title and len(title) > 10:
                key_points.append(title[:100])
        
        return key_points
    
    def _fallback_batch_summary(self, contents: List[Dict]) -> str:
        """R√©sum√© de secours bas√© sur des r√®gles simples"""
        
        total = len(contents)
        authors = set(c.get('author', 'Anonyme') for c in contents)
        
        # Analyser les sentiments
        sentiments = [c.get('sentiment') for c in contents if c.get('sentiment')]
        sentiment_counts = {}
        for s in sentiments:
            sentiment_counts[s] = sentiment_counts.get(s, 0) + 1
        
        dominant_sentiment = max(sentiment_counts, key=sentiment_counts.get) if sentiment_counts else 'neutre'
        
        # Compter les mots-cl√©s fr√©quents
        from collections import Counter
        all_text = ' '.join([
            f"{c.get('title', '')} {c.get('content', '')}"
            for c in contents
        ]).lower()
        
        # Mots communs √† ignorer
        stop_words = {'le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'ou', 'mais', 'est', 'sont', 'a', 'the', 'and', 'or', 'is', 'are'}
        words = [w for w in all_text.split() if len(w) > 4 and w not in stop_words]
        common_words = Counter(words).most_common(3)
        
        summary = f"Analyse de {total} contenus de {len(authors)} auteur(s). "
        summary += f"Ton g√©n√©ral: {dominant_sentiment}. "
        
        if common_words:
            themes = ', '.join([word for word, count in common_words])
            summary += f"Th√®mes r√©currents: {themes}."
        
        return summary
    
    def _aggregate_sentiments(self, contents: List[Dict]) -> Dict:
        """Agr√©ger les sentiments de tous les contenus"""
        
        sentiment_counts = {'positive': 0, 'neutral': 0, 'negative': 0, 'unknown': 0}
        
        for content in contents:
            sentiment = content.get('sentiment', 'unknown')
            if sentiment in sentiment_counts:
                sentiment_counts[sentiment] += 1
            else:
                sentiment_counts['unknown'] += 1
        
        total = len(contents)
        
        return {
            'distribution': sentiment_counts,
            'percentages': {
                k: round((v / total) * 100, 1) if total > 0 else 0
                for k, v in sentiment_counts.items()
            },
            'dominant': max(sentiment_counts, key=sentiment_counts.get),
            'total_analyzed': total
        }
    
    def _extract_themes(self, batches: List[ContentBatch]) -> List[str]:
        """Extraire les th√®mes dominants des r√©sum√©s de lots"""
        
        # Combiner tous les r√©sum√©s
        all_summaries = ' '.join([b.summary for b in batches if b.summary])
        
        # Extraire les mots-cl√©s fr√©quents (simple)
        from collections import Counter
        
        stop_words = {
            'le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'ou', 'mais',
            'est', 'sont', 'a', 'ont', 'pour', 'dans', 'sur', 'avec', 'par',
            'the', 'and', 'or', 'is', 'are', 'of', 'to', 'in', 'for', 'with'
        }
        
        words = [
            w.lower() for w in all_summaries.split()
            if len(w) > 4 and w.lower() not in stop_words
        ]
        
        # Les 5 mots les plus fr√©quents = th√®mes
        common = Counter(words).most_common(5)
        themes = [word.capitalize() for word, count in common if count >= 2]
        
        return themes
    
    async def _generate_final_synthesis(
        self,
        batches: List[ContentBatch],
        sentiment_aggregate: Dict,
        themes: List[str],
        context: str
    ) -> Tuple[str, List[str]]:
        """
        G√©n√©rer la synth√®se finale √† partir des r√©sum√©s de lots
        
        C'est l'√©tape finale qui cr√©e le rapport narratif
        """
        
        # Combiner les r√©sum√©s de lots
        batch_summaries_text = '\n\n'.join([
            f"Lot {b.batch_id}: {b.summary}"
            for b in batches if b.summary
        ])
        
        sentiment_summary = (
            f"{sentiment_aggregate['percentages']['positive']:.0f}% positif, "
            f"{sentiment_aggregate['percentages']['neutral']:.0f}% neutre, "
            f"{sentiment_aggregate['percentages']['negative']:.0f}% n√©gatif"
        )
        
        prompt = f"""Tu es un analyste strat√©gique. Synth√©tise ces r√©sum√©s en un rapport narratif coh√©rent.

CONTEXTE: {context}
CONTENUS ANALYS√âS: {sentiment_aggregate['total_analyzed']}
SENTIMENT GLOBAL: {sentiment_summary}
TH√àMES: {', '.join(themes)}

R√âSUM√âS PAR LOT:
{batch_summaries_text}

INSTRUCTIONS:
R√©dige une synth√®se narrative en 5-7 paragraphes qui:
1. Pr√©sente la situation globale
2. Analyse les tendances et opinions dominantes
3. Identifie les pr√©occupations r√©currentes
4. √âvalue le ton et l'engagement
5. Conclut avec les insights principaux

Style: Professionnel, factuel, paragraphes fluides (PAS de listes √† puces).

SYNTH√àSE:"""
        
        try:
            synthesis = await self.llm_service.analyze_with_local_llm(
                prompt,
                {'total_batches': len(batches)}
            )
            
            # Nettoyer
            synthesis = self._clean_summary(synthesis)
            
            # Extraire les insights cl√©s
            key_insights = self._extract_final_insights(batches, sentiment_aggregate, themes)
            
            return synthesis, key_insights
            
        except Exception as e:
            logger.error(f"Erreur synth√®se finale: {e}")
            # Fallback
            return self._fallback_final_synthesis(
                batches, sentiment_aggregate, themes, context
            )
    
    def _extract_final_insights(
        self,
        batches: List[ContentBatch],
        sentiment_aggregate: Dict,
        themes: List[str]
    ) -> List[str]:
        """Extraire les insights cl√©s de l'analyse"""
        
        insights = []
        
        # Insight sur le volume
        total = sentiment_aggregate['total_analyzed']
        insights.append(f"{total} contenus analys√©s sur {len(batches)} sources")
        
        # Insight sur le sentiment
        neg_ratio = sentiment_aggregate['percentages']['negative']
        if neg_ratio > 60:
            insights.append(f"‚ö†Ô∏è Sentiment critique dominant ({neg_ratio:.0f}% n√©gatif)")
        elif neg_ratio < 20:
            insights.append(f"‚úÖ Sentiment majoritairement positif")
        
        # Insight sur les th√®mes
        if themes:
            insights.append(f"Th√®mes principaux: {', '.join(themes[:3])}")
        
        # Insight sur l'engagement
        all_contents = []
        for batch in batches:
            all_contents.extend(batch.contents)
        
        if all_contents:
            avg_engagement = sum(c.get('engagement_score', 0) for c in all_contents) / len(all_contents)
            high_engagement_count = len([c for c in all_contents if c.get('engagement_score', 0) > avg_engagement * 2])
            
            if high_engagement_count > len(all_contents) * 0.2:
                insights.append(f"Forte viralit√© d√©tect√©e ({high_engagement_count} contenus tr√®s engageants)")
        
        return insights[:5]  # Top 5 insights
    
    def _fallback_final_synthesis(
        self,
        batches: List[ContentBatch],
        sentiment_aggregate: Dict,
        themes: List[str],
        context: str
    ) -> Tuple[str, List[str]]:
        """Synth√®se de secours bas√©e sur des r√®gles"""
        
        total = sentiment_aggregate['total_analyzed']
        neg_pct = sentiment_aggregate['percentages']['negative']
        pos_pct = sentiment_aggregate['percentages']['positive']
        
        synthesis = f"L'analyse de {total} contenus sur '{context}' r√©v√®le les √©l√©ments suivants. "
        
        if neg_pct > 50:
            synthesis += f"Le ton est majoritairement critique ({neg_pct:.0f}% de sentiment n√©gatif), "
            synthesis += "refl√©tant des pr√©occupations marqu√©es au sein de l'opinion surveill√©e. "
        elif pos_pct > 50:
            synthesis += f"Le ton est globalement favorable ({pos_pct:.0f}% de sentiment positif). "
        else:
            synthesis += "Le ton reste partag√© entre opinions positives et n√©gatives. "
        
        if themes:
            synthesis += f"Les th√®mes dominants identifi√©s sont: {', '.join(themes)}. "
        
        synthesis += f"Cette analyse couvre {len(batches)} sources distinctes collect√©es sur la p√©riode de surveillance."
        
        insights = self._extract_final_insights(batches, sentiment_aggregate, themes)
        
        return synthesis, insights
    
    def _empty_summary(self) -> HierarchicalSummary:
        """Retourner un r√©sum√© vide"""
        return HierarchicalSummary(
            final_summary="Aucun contenu √† analyser.",
            key_insights=[],
            sentiment_analysis={'distribution': {}, 'percentages': {}, 'dominant': 'unknown', 'total_analyzed': 0},
            themes=[],
            batch_summaries=[],
            total_contents_analyzed=0,
            processing_time=0.0
        )


# Fonction utilitaire pour estimer la taille d'un prompt
def estimate_token_count(text: str) -> int:
    """
    Estimer le nombre de tokens (approximatif)
    1 token ‚âà 4 caract√®res en moyenne
    """
    return len(text) // 4


# Exemple d'utilisation
async def demo_hierarchical_summarization():
    """D√©monstration du r√©sum√© hi√©rarchique"""
    
    # Simuler un grand dataset
    fake_contents = [
        {
            'title': f'Post {i}',
            'content': f'Contenu du post {i}' * 10,
            'author': f'User{i % 10}',
            'sentiment': ['positive', 'neutral', 'negative'][i % 3],
            'engagement_score': (i * 13) % 1000
        }
        for i in range(100)  # 100 contenus
    ]
    
    # Cr√©er un mock LLM service
    class MockLLMService:
        async def analyze_with_local_llm(self, prompt, context):
            # Simuler une r√©ponse
            await asyncio.sleep(0.5)  # Simuler le temps de traitement
            return "R√©sum√© g√©n√©r√© par le LLM."
    
    mock_llm = MockLLMService()
    summarizer = HierarchicalSummarizer(mock_llm, batch_size=20)
    
    result = await summarizer.summarize_large_dataset(
        fake_contents,
        context="test de r√©sum√© hi√©rarchique"
    )
    
    print(f"\n‚úÖ R√©sum√© termin√©:")
    print(f"   - {result.total_contents_analyzed} contenus analys√©s")
    print(f"   - {len(result.batch_summaries)} lots trait√©s")
    print(f"   - Temps: {result.processing_time:.1f}s")
    print(f"\nüìä Synth√®se finale:")
    print(f"   {result.final_summary}")


if __name__ == '__main__':
    asyncio.run(demo_hierarchical_summarization())