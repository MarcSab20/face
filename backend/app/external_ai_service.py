"""
Service d'IA Externe Gratuit
Int√®gre Gemini (Google) et Groq pour des synth√®ses de qualit√© sup√©rieure
"""

import logging
import asyncio
import aiohttp
from typing import Dict, Optional, List
from datetime import datetime, timedelta
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class APIQuota:
    """Suivi du quota d'une API"""
    requests_made: int
    requests_limit: int
    reset_at: datetime
    is_available: bool


class ExternalAIService:
    """
    Service d'IA externe gratuit
    
    APIs gratuites disponibles:
    - Google Gemini: 15 req/min (60 req/h) gratuit
    - Groq: 30 req/min gratuit  
    
    G√®re intelligemment les quotas et bascule entre les services
    """
    
    def __init__(
        self,
        gemini_api_key: Optional[str] = None,
        groq_api_key: Optional[str] = None
    ):
        self.gemini_api_key = gemini_api_key
        self.groq_api_key = groq_api_key
        
        # Quotas
        self.gemini_quota = APIQuota(0, 15, datetime.utcnow() + timedelta(minutes=1), bool(gemini_api_key))
        self.groq_quota = APIQuota(0, 30, datetime.utcnow() + timedelta(minutes=1), bool(groq_api_key))
        
        # URLs
        self.gemini_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"
        self.groq_url = "https://api.groq.com/openai/v1/chat/completions"
        
        self.session = None
        
        # Log des services disponibles
        available = []
        if gemini_api_key:
            available.append("Gemini")
        if groq_api_key:
            available.append("Groq")
        
        if available:
            logger.info(f"‚úÖ Services IA externes disponibles: {', '.join(available)}")
        else:
            logger.warning("‚ö†Ô∏è Aucun service IA externe configur√©. Utilisation des LLM locaux uniquement.")
    
    async def __aenter__(self):
        """Context manager entry"""
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        if self.session:
            await self.session.close()
    
    async def generate_smart_synthesis(
        self,
        prompt: str,
        context_data: Dict,
        max_tokens: int = 1000,
        temperature: float = 0.3
    ) -> Dict:
        """
        G√©n√©rer une synth√®se intelligente en choisissant le meilleur service disponible
        
        Args:
            prompt: Prompt pour le LLM
            context_data: Donn√©es contextuelles
            max_tokens: Nombre max de tokens
            temperature: Temp√©rature (0-1)
            
        Returns:
            Dict avec la r√©ponse et les m√©tadonn√©es
        """
        
        # Choisir le meilleur service disponible
        service = self._select_best_service()
        
        if service == 'gemini':
            return await self._generate_with_gemini(prompt, max_tokens, temperature)
        elif service == 'groq':
            return await self._generate_with_groq(prompt, max_tokens, temperature)
        else:
            return {
                'text': None,
                'service': 'none',
                'error': 'Aucun service IA externe disponible'
            }
    
    def _select_best_service(self) -> str:
        """S√©lectionner le meilleur service selon la disponibilit√© et les quotas"""
        
        now = datetime.utcnow()
        
        # Reset des quotas si n√©cessaire
        if now >= self.gemini_quota.reset_at:
            self.gemini_quota.requests_made = 0
            self.gemini_quota.reset_at = now + timedelta(minutes=1)
        
        if now >= self.groq_quota.reset_at:
            self.groq_quota.requests_made = 0
            self.groq_quota.reset_at = now + timedelta(minutes=1)
        
        # V√©rifier Gemini
        if (self.gemini_quota.is_available and 
            self.gemini_quota.requests_made < self.gemini_quota.requests_limit):
            return 'gemini'
        
        # Sinon Groq
        if (self.groq_quota.is_available and 
            self.groq_quota.requests_made < self.groq_quota.requests_limit):
            return 'groq'
        
        return 'none'
    
    async def _generate_with_gemini(
        self,
        prompt: str,
        max_tokens: int,
        temperature: float
    ) -> Dict:
        """
        G√©n√©rer avec Google Gemini API
        
        Gemini 1.5 Flash: gratuit, rapide, bon pour r√©sum√©s
        """
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        try:
            url = f"{self.gemini_url}?key={self.gemini_api_key}"
            
            payload = {
                "contents": [{
                    "parts": [{
                        "text": prompt
                    }]
                }],
                "generationConfig": {
                    "temperature": temperature,
                    "maxOutputTokens": max_tokens,
                    "topP": 0.95,
                    "topK": 40
                }
            }
            
            async with self.session.post(url, json=payload, timeout=30) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    # Extraire le texte de la r√©ponse
                    text = data['candidates'][0]['content']['parts'][0]['text']
                    
                    # Incr√©menter le quota
                    self.gemini_quota.requests_made += 1
                    
                    logger.info(f"‚úÖ Gemini: {len(text)} caract√®res g√©n√©r√©s (quota: {self.gemini_quota.requests_made}/{self.gemini_quota.requests_limit})")
                    
                    return {
                        'text': text,
                        'service': 'gemini',
                        'model': 'gemini-1.5-flash',
                        'tokens_used': len(text) // 4,  # Approximation
                        'success': True
                    }
                else:
                    error_text = await response.text()
                    logger.error(f"Erreur Gemini {response.status}: {error_text}")
                    return {'text': None, 'service': 'gemini', 'error': error_text}
                    
        except asyncio.TimeoutError:
            logger.error("Timeout Gemini API")
            return {'text': None, 'service': 'gemini', 'error': 'Timeout'}
        except Exception as e:
            logger.error(f"Erreur Gemini: {e}")
            return {'text': None, 'service': 'gemini', 'error': str(e)}
    
    async def _generate_with_groq(
        self,
        prompt: str,
        max_tokens: int,
        temperature: float
    ) -> Dict:
        """
        G√©n√©rer avec Groq API
        
        Groq: Tr√®s rapide, gratuit, bon pour r√©sum√©s courts
        """
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        try:
            headers = {
                'Authorization': f'Bearer {self.groq_api_key}',
                'Content-Type': 'application/json'
            }
            
            payload = {
                "model": "llama-3.1-8b-instant",  # Mod√®le rapide
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": temperature,
                "max_tokens": max_tokens,
                "top_p": 0.95
            }
            
            async with self.session.post(
                self.groq_url,
                headers=headers,
                json=payload,
                timeout=30
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    text = data['choices'][0]['message']['content']
                    
                    # Incr√©menter le quota
                    self.groq_quota.requests_made += 1
                    
                    logger.info(f"‚úÖ Groq: {len(text)} caract√®res g√©n√©r√©s (quota: {self.groq_quota.requests_made}/{self.groq_quota.requests_limit})")
                    
                    return {
                        'text': text,
                        'service': 'groq',
                        'model': 'llama-3.1-8b-instant',
                        'tokens_used': data['usage']['total_tokens'],
                        'success': True
                    }
                else:
                    error_text = await response.text()
                    logger.error(f"Erreur Groq {response.status}: {error_text}")
                    return {'text': None, 'service': 'groq', 'error': error_text}
                    
        except asyncio.TimeoutError:
            logger.error("Timeout Groq API")
            return {'text': None, 'service': 'groq', 'error': 'Timeout'}
        except Exception as e:
            logger.error(f"Erreur Groq: {e}")
            return {'text': None, 'service': 'groq', 'error': str(e)}
    
    def get_quota_status(self) -> Dict:
        """Obtenir le statut des quotas"""
        return {
            'gemini': {
                'available': self.gemini_quota.is_available,
                'used': self.gemini_quota.requests_made,
                'limit': self.gemini_quota.requests_limit,
                'reset_in_seconds': (self.gemini_quota.reset_at - datetime.utcnow()).seconds
            },
            'groq': {
                'available': self.groq_quota.is_available,
                'used': self.groq_quota.requests_made,
                'limit': self.groq_quota.requests_limit,
                'reset_in_seconds': (self.groq_quota.reset_at - datetime.utcnow()).seconds
            }
        }
    
    async def generate_executive_summary(
        self,
        batch_summaries: List[str],
        sentiment_data: Dict,
        themes: List[str],
        context: str,
        total_contents: int
    ) -> str:
        """
        G√©n√©rer un r√©sum√© ex√©cutif de haute qualit√©
        
        Utilise le meilleur mod√®le disponible pour une synth√®se professionnelle
        """
        
        sentiment_summary = (
            f"{sentiment_data['percentages']['positive']:.0f}% positif, "
            f"{sentiment_data['percentages']['neutral']:.0f}% neutre, "
            f"{sentiment_data['percentages']['negative']:.0f}% n√©gatif"
        )
        
        prompt = f"""Tu es un analyste strat√©gique senior. R√©dige un r√©sum√© ex√©cutif professionnel.

CONTEXTE: {context}
VOLUME: {total_contents} contenus analys√©s
SENTIMENT GLOBAL: {sentiment_summary}
TH√àMES DOMINANTS: {', '.join(themes)}

R√âSUM√âS PAR SECTION:
{chr(10).join([f"Section {i+1}: {summary}" for i, summary in enumerate(batch_summaries)])}

INSTRUCTIONS CRITIQUES:
R√©dige un r√©sum√© ex√©cutif en 5-7 paragraphes fluides et narratifs (PAS de listes √† puces).

Le r√©sum√© doit:
1. Commencer par une vue d'ensemble de la situation
2. Analyser les tendances et opinions dominantes
3. Identifier les pr√©occupations ou opportunit√©s
4. √âvaluer le niveau de risque ou d'engagement
5. Conclure avec les insights strat√©giques principaux

Style: Professionnel, factuel, paragraphes r√©dig√©s, fran√ßais soutenu.
Ton: Neutre et objectif, adapt√© √† un briefing minist√©riel.

R√âSUM√â EX√âCUTIF:"""
        
        result = await self.generate_smart_synthesis(
            prompt,
            context_data={'total_contents': total_contents},
            max_tokens=800,
            temperature=0.2  # Basse temp√©rature pour plus de factuel
        )
        
        if result.get('success') and result.get('text'):
            logger.info(f"‚úÖ R√©sum√© ex√©cutif g√©n√©r√© via {result['service'].upper()}")
            return result['text']
        else:
            logger.warning("‚ö†Ô∏è √âchec g√©n√©ration via API externe, utilisation fallback")
            return self._fallback_executive_summary(
                batch_summaries, sentiment_data, themes, context, total_contents
            )
    
    def _fallback_executive_summary(
        self,
        batch_summaries: List[str],
        sentiment_data: Dict,
        themes: List[str],
        context: str,
        total_contents: int
    ) -> str:
        """R√©sum√© ex√©cutif de secours bas√© sur des r√®gles"""
        
        neg_pct = sentiment_data['percentages']['negative']
        pos_pct = sentiment_data['percentages']['positive']
        
        summary = f"L'analyse de {total_contents} contenus sur '{context}' r√©v√®le les √©l√©ments suivants. "
        
        # Paragraphe 1: Sentiment global
        if neg_pct > 60:
            summary += f"Le sentiment global est majoritairement critique ({neg_pct:.0f}% n√©gatif), "
            summary += "refl√©tant des pr√©occupations marqu√©es au sein de l'opinion surveill√©e. "
        elif pos_pct > 60:
            summary += f"Le sentiment global est favorable ({pos_pct:.0f}% positif), "
            summary += "indiquant une r√©ception g√©n√©ralement positive. "
        else:
            summary += "Le sentiment reste partag√© entre opinions positives et critiques, "
            summary += "sugg√©rant une polarisation de l'opinion publique. "
        
        # Paragraphe 2: Th√®mes
        if themes:
            summary += f"\n\nLes th√®mes dominants identifi√©s sont: {', '.join(themes)}. "
            summary += "Ces sujets concentrent la majorit√© des discussions et refl√®tent les pr√©occupations actuelles. "
        
        # Paragraphe 3: Volume et sources
        summary += f"\n\nCette analyse couvre {len(batch_summaries)} sources distinctes collect√©es sur la p√©riode de surveillance. "
        summary += "La distribution des contenus indique une activit√© soutenue sur l'ensemble des plateformes monitor√©es. "
        
        # Paragraphe 4: Conclusion
        if neg_pct > 50:
            summary += "\n\nLa pr√©pond√©rance du sentiment critique n√©cessite une attention particuli√®re et pourrait justifier des actions de communication corrective."
        else:
            summary += "\n\nLa situation demeure sous contr√¥le et ne requiert pas d'action imm√©diate, bien qu'une surveillance continue soit recommand√©e."
        
        return summary


# Fonctions utilitaires pour obtenir les cl√©s API
def get_gemini_setup_instructions() -> str:
    """Instructions pour obtenir une cl√© Gemini gratuitement"""
    return """
    üîë Comment obtenir une cl√© API Gemini GRATUITE:
    
    1. Aller sur: https://makersuite.google.com/app/apikey
    2. Se connecter avec un compte Google
    3. Cliquer sur "Create API key"
    4. Copier la cl√© g√©n√©r√©e
    5. Ajouter dans .env: GEMINI_API_KEY=votre_cle
    
    Limites gratuites:
    - 15 requ√™tes/minute
    - 1 million tokens/mois
    - Id√©al pour les r√©sum√©s !
    """


def get_groq_setup_instructions() -> str:
    """Instructions pour obtenir une cl√© Groq gratuitement"""
    return """
    üîë Comment obtenir une cl√© API Groq GRATUITE:
    
    1. Aller sur: https://console.groq.com
    2. Cr√©er un compte (gratuit)
    3. Aller dans "API Keys"
    4. Cr√©er une nouvelle cl√©
    5. Copier la cl√© g√©n√©r√©e
    6. Ajouter dans .env: GROQ_API_KEY=votre_cle
    
    Limites gratuites:
    - 30 requ√™tes/minute
    - Tr√®s rapide !
    - Parfait pour les analyses
    """


# Test du service
async def test_external_ai_service():
    """Tester les services IA externes"""
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    
    gemini_key = os.getenv('GEMINI_API_KEY')
    groq_key = os.getenv('GROQ_API_KEY')
    
    if not gemini_key and not groq_key:
        print("‚ùå Aucune cl√© API configur√©e")
        print("\n" + get_gemini_setup_instructions())
        print("\n" + get_groq_setup_instructions())
        return
    
    async with ExternalAIService(gemini_key, groq_key) as ai_service:
        
        print("\nüîç Test de g√©n√©ration de r√©sum√©...")
        
        test_prompt = """R√©sume en 3 phrases:
        L'intelligence artificielle transforme le monde du travail. 
        De nombreuses entreprises adoptent l'IA pour am√©liorer leur productivit√©.
        Cependant, des questions √©thiques se posent sur l'automatisation.
        """
        
        result = await ai_service.generate_smart_synthesis(
            test_prompt,
            context_data={},
            max_tokens=200
        )
        
        if result.get('success'):
            print(f"\n‚úÖ Service utilis√©: {result['service'].upper()}")
            print(f"üìù R√©sum√© g√©n√©r√©:\n{result['text']}")
        else:
            print(f"\n‚ùå √âchec: {result.get('error')}")
        
        # Afficher les quotas
        print("\nüìä Statut des quotas:")
        quotas = ai_service.get_quota_status()
        for service, info in quotas.items():
            if info['available']:
                print(f"   {service.upper()}: {info['used']}/{info['limit']} requ√™tes")


if __name__ == '__main__':
    asyncio.run(test_external_ai_service())